[환경 구성](#환경-구성)

1. [JDK & Maven 설치](#JDK-&-Maven-설치)

[스프링 마이크로서비스 코딩 공작소](#스프링-마이크로서비스-코딩-공작소)

1. [스프링, 클라우드와 만나다](#spmia-chapter1)

1. [스프링 부트로 마이크로서비스 구축](#spmia-chapter2)

1. [스프링 클라우드 컨피그 서버로 구성관리](#spmia-chapter3)

1. [스프링 클라우드와 주울로 서비스 라우팅](#spmia-chapter6)
   * [주울과 서비스 타임아웃](#주울과-서비스-타임아웃)
   * [주울의 진정한 힘! 필터](#주울의-진정한-힘!-필터)
   * [동적 경로 필터 작성](#동적-경로-필터-작성)

1. [마이크로서비스의 보안](#spmia-chapter7)

1. [spmia-chapter8](#spmia-chapter8)

1. [스프링 클라우드 슬루스와 집킨을 이용한 분산 추적](#spmia-chapter9)


1. [서비스 디스커버리](#서비스-디스커버리)
   1. [디스커버리 클라이언트 - 유레카 클라이언트](#디스커버리-클라이언트---유레카-클라이언트) 
   1. [서비스 디스커버리를 사용해 서비스 검색](#서비스-디스커버리를-사용해-서비스-검색)

1. [스프링 클라우드 슬루스와 집킨을 이용한 분산 추적](#스프링-클라우드-슬루스와-집킨을-이용한-분산-추적)

1. [Linux 자바 설치](#Linux 자바 설치)
1. [Docker 로 Kafka 설치하기](#Docker로-Kafka-설치하기)

# 환경 구성

## ubuntu 22.04

 * 개발환경

   ```shell
   $> lsb_release -a
   ```

 * 가상머신

    ```shell
    $> virt-install --name t.149 --vcpus 2 --memory 4096 \
        --os-type linux --os-variant ubuntu22.04 \
        --disk path=/data/vm/imgs/ubuntu22.04.t.149.qcow2,format=qcow2,device=disk,bus=virtio \
        --network network=bridged,model=virtio \
        --boot hd --graphics spice,listen=0.0.0.0

    $> virt-install --name t.148 --vcpus 2 --memory 4096 \
        --os-type linux --os-variant ubuntu22.04 \
        --disk path=/data/vm/imgs/ubuntu22.04.t.148.qcow2,format=qcow2,device=disk,bus=virtio \
        --network network=bridged,model=virtio \
        --boot hd --graphics spice,listen=0.0.0.0

    $> sudo hostnamectl set-hostname t149
    spice://172.31.37.8:5900
    ```

## VS Code git 연동

## 인텔리제이 설치

## JDK & Maven 설치

## Docker 설치

 * 패키지 업데이트 및 필요한 패키지 설치
 
   ```shell
   $> sudo apt update &
      sudo apt install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common
   ```

* Docker 설치

   ```shell
   # Docker의 공식 GPG 키 추가
   $> curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

   # Docker의 공식 apt 저장소 추가
   $> sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

   # 패키지 업데이트
   $> sudo apt update &&
      sudo apt -y install docker-ce docker-ce-cli containerd.io

   $> sudo usermod -aG docker $USER
   ```

* Docker-compose 설치

   ```shell
   $> curl -s https://api.github.com/repos/docker/compose/releases/latest | grep tag_name

   $> sudo curl -sSL "https://github.com/docker/compose/releases/download/$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep tag_name | cut -d '"' -f 4)/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

   $> curl -fsSL https://github.com/docker/compose/releases/download/$(curl -s https://api.github.com/repos/docker/compose/releases/latest \
   | sed 's#.*tag/\(.*\)\".*#\1#')/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose &&
   chmod +x /usr/local/bin/docker-compose &&
   ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

   $> sudo chmod +x /usr/local/bin/docker-compose
   $> docker-compose --version
   ```

# 스프링 마이크로서비스 코딩 공작소

 > https://github.com/gilbutITbook/006962.git  
 > https://github.com/klimtever

## spmia-chapter1

 > 스프링, 클라우드와 만나다  
   https://github.com/klimtever/spmia-chapter1.git

## spmia-chapter2

 > 스프링 부트로 마이크로서비스 구축  
   https://github.com/klimtever/spmia-chapter2.git

 * 마이크로서비스 아키텍처를 구축할 때 세분화 문제
   1. 큰 마이크로서비스에서 시작해 작게 리팩토링하는 것이 더 낫다.
   1. 서비스 간 교류하는 방식에 먼저 집중한다.
   1. 문제 영역에 대한 이해가 깊어짐에 따라 서비스 책임도 계속 변한다.

 - 나쁜 마이크로서비스의 징후
   1. 책임이 너무 많은 서비스
   1. 많은 테이블의 데이터를 관리하는 서비스
   1. 과다한 테스트 케이스
   1. 한 문제 영역 부분에 속한 마이크로서비스가 토끼처럼 번식하낟.
   1. 마이크로서비스가 지나치게 상호 의존적이다.
   1. 마이크로서비스가 단순한 CRUD 집하이 된다.

  * 마이크로서비스 아키텍처는 처음부터 올바르게 설계하기가 어렵기 때문에 진화론적 사고 과정으로 개발해야 한다.

## spmia-chapter3

  Spring Cloud Config 는 분산 시스템에서 설정 정보를 외부에 보관할 수 있도록 지원해주는 서비스이다.

  ![Alt text](img1.daumcdn.png)

 > 스프링 클라우드 컨피그 서버로 구성관리  
   https://github.com/klimtever/spmia-chapter3.git

 * confsvr/src/main/docker/Dockerfile

   ```shell
   RUN cd /tmp/ && \
       curl -k -LO "http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip" -H 'Cookie: oraclelicense=accept-securebackup-cookie' && \
       unzip jce_policy-8.zip && \
       rm jce_policy-8.zip && \
       yes |cp -v /tmp/UnlimitedJCEPolicyJDK8/*.jar /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/
   ADD @project.build.finalName@.jar /usr/local/configserver/
   ```
   위와 같은 경우 docker 이미지 빌드시 오류가 발생(아래와 같이 수정)

   ```shell
   RUN cd /tmp/ && \
       curl -L -b "oraclelicense=a" http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip -O && \
       unzip jce_policy-8.zip && \
       rm jce_policy-8.zip && \
       yes |cp -v /tmp/UnlimitedJCEPolicyJDK8/*.jar /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/
   ADD @project.build.finalName@.jar /usr/local/configserver/
   ```

 * JAVA 명령창 다운로드

    ```shell
    curl -O -v -j -k -L -H "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gz

    wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gz
    ```

 * 구성관리서버(Spring Cloud Config Server)

   Spring Cloud Config Server는 Config 저장소와 연결되어 중간 역할을 수행한다."Config Server" 의존성을 추가한다.

    ```java
    @EnableConfigServer
    ```

   - 구성 데이터를 보관할 백엔드저장소 지정

     ```yaml
     spring.cloud.config.server:
       encrypt:
         enabled: false # Config Server 에서 복호화 비활성화
       git:
         uri: file://D:/__test/spring-cloud-config-repo
         username:
         password:

        # Spring Cloud Config 는 Remote에 있는 Git Reposityory 를 클론해온다.
        # 하지만 이전에 클론해와서 로컬에 있는 정보의 폴더구조가 변경되는 등의 문제가 발생하면 제대로 업데이트할 수 없는데,
        # 이러한 상황에서 강제로 Git Repository를 클론하기 위한 설정이다.
        force-pull: true
        timeout: 30

        # Config 저장소 상에서 설정 파일을 탐색할 경로를 지정하는 설정
        # {application} 에는 Client의 spring.application.name 값이 사용된다.
        searchPaths:
          - '{application}'
          - '{application}/*'

       native:
         searchLocations: file://.../config/...,
                          file://.../config/...
         #searchLocations: classpath:config, classpath:config/...
     encrypt:
       key: my_config_key
     ```

 - Spring Cloud Config Server Endpoint

    > GET /{application}/{profile}[/{label}]  
    > GET /{application}-{profile}.yml  
    > GET /{label}/{application}-{profile}.yml  
    > GET /{application}-{profile}.properties  
    > GET /{label}/{application}-{profile}.properties

   - application : Spring Cloud Config Client의 application.properties(application.yml)파일에 정의한 spring.application.name 속성 값을 의미한다.
   - profile : Spring Cloud Config Client의 application.properties (application.yml) 파일에 정의한 spring.profiles.active 속성 값을 의미한다.
   - label : Optional 값으로, Git branch 정보를 의미한다.

<br>

 * Spring Cloud Config Client  

   클라이언트 프로젝트를 생성할 때, "Config Client", "Spring Boot Actuator"의 존성을 추가한다.

   Spring Cloud Config 기능 중 하나인 ```@RefreshScope```는 config파일의 데이터가 변경되면 서버를 재실행 해주는 기능이다.

    ```yaml
    spring:
      application:
        # 어플리케이션 이름 = Github 레포지토리에서 관리하는 파일 애플리케이션 이름
        name: config
      profiles:
        active: local     # 애플리케이션 환경 = local, dev
     config:
       import: optional:configserver:http://localhost:8088 # import:optional:configserver사용 -> Spring cloud config server 주소 
    server:
      port: 9001     # Spring Cloud Config Client 주소

    # 클라이언트 코드에서 Actuator 설정
    management:
      endpoints:
        web:
          exposure:
            include: "*"
    ```

    ```yaml
    spring:
      profiles: dev

      cloud:
        config:
          uri: "http://config.ch4njun.com:8888"
          profile: ${spring.profiles.active}, sample-${spring.profiles.active}
          label: develop
    ---
    spring:
      profiles: qa

      cloud:
        config:
          uri: "http://config.ch4njun.com:8888"
          profile: ${spring.profiles.active}, sample-${spring.profiles.active}
          label: develop
    ```
    spring.cloud.config.profie은 Spring Cloud Config Server에 전달할 Profiles 목록이다!  
    Server는 해당 목록에 해당하는 모든 설정 파일을 Client에게 보내준다.

 * spring-boot-starter-actuator

   actuator의 기본적인 역할은 해당 애플리케이션의 상태를 종합적으로 정리해서 Client에게 제공해주는 역할을 수행한다.

   /actuator/health, /actuator/env, /actuator/info 등 다양한 API로 애플리케이션의 정보를 제공한다.

    ```yaml
    # actuator 기본 설정
    management:
      endpoint:
        health:
          show-details: never
      endpoints:
        web:
          base-path: /servicemanager
          exposure:
            include: "*"
    ```

    /actuator/refresh 는 애플리케이션의 설정 파일을 다시 로딩하는 역할을 수행한다.

 * spring cloud config 암호화 설정

   Spring Cloud Config에서는 암호화 할 수 있는 종단점을 기본적으로 제공합니다.  
   POST 형태의 /encrypt 와 /descrypt 이며 key store를 등록해야 사용 가능합니다.

   - 대칭키를 설정하는 두가지 방법

     컨피그 서버의 application.yml or properties 파일에 encrypt.key = (대칭키)

     시스템 환경 변수에  "ENCRYPT_KEY = (대칭키)

   - client-server -> 의존성 추가

     spring-security-rsa를 추가하면 암호화 된 값을 자동으로 복호화 합니다.
     
     ```
     implementation 'org.springframework.security:spirng-security-rsa'
     implementation 'org.springframework.cloud:spring-cloud-starter-bootstrip'
     ```

   - client-client -> /actuator/env 조회  

     복호화 된 비밀번호는 *로 마스킹 처리되어 표시됩니다.

 ----
 <br>

 * postgress:9.5 도커 이미지 수동 실행

   ```shell
   $> docker pull postgres:9.5
   $> docker images
   $> docker run --name db_9.5           \
      -p "5432:5432"                     \
      -e POSTGRES_USER="postgres"        \
      -e POSTGRES_PASSWORD="p0stgr@s"    \
      -e POSTGRES_DB="eagle_eye_local"   \
      -d postgres:9.5
   ```

   ```shell
   $> https://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip
   $> curl -L -b "oraclelicense=a" https://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip -O

   $> vi $JAVA_HOME/jre/lib/security/java.security
      crypto.policy=unlimited
   $> java -jar ./confsvr/target/configurationserver-0.0.1-SNAPSHOT.jar
   $> java -Dspring.cloud.config.uri=http://172.31.37.149:8888 \
           -Dspring.profiles.active=default                    \
           -jar ./licensing-service/target/licensing-service-0.0.1-SNAPSHOT.jar
   ```
   ----

  * 암호화 키 설정

    스프링 클라우드 컨피그 서버에서 사용되는 대칭 암호화 키는 ENCRYPT_KEY라는 운영 체제의 환경 변수를 사용해 서비스에 전다뢰는 선택된 문자열이다.

## spmia-chapter6

> 스프링 클라우드와 주울로 서비스 라우팅
  https://github.com/klimtever/spmia-chapter6.git


  * 마이크로서비스와 함께 서비스 게이트웨이 사용
  * 스프링 클라우드와 네플릭스 주울(Zuul)을 사용한 서비스 게이트웨이 구형
  * 주울로 마이크로서비스 경로 매핑
  * 상관관계 ID와 추적을 위한 필터 생성
  * 주울을 이요한 동적 라우팅

### 서비스 게이트웨이란?

  서비스 게이트웨이는 서비스 클라이언트와 호출될 서비스 사이에서 중개 역할을 한다. 서비스 클라이언트는 서비스 게이트웨이가 관리하는 하나의 URL을 통해 통신한다.  

  서비스 게이트웨이는 애플리케이션 안의 마이크로서비스 호출로 유입되는 모든 트래픽에 대해 게이트키퍼 역할을 한다.

  서비스 게이트웨이는 클라이언트가 각 서비스에 보내는 모든 호출 사이에 위치하므로 게이트웨이는 서비스 호출에 대한 **중앙 집중식 정책 시행 지점(PEP)** 역할을 한다(서비스의 횡단 관심사를 단일 지점에서 구현할 수 있다).

* 횡단 관심사
  1. 정적 라우팅
  1. 동적 라우팅
  1. 인증(authentication)과 인가()
  1. 트랜잭션 처리
  1. 에러처리

오픈소스 API Gateway에는 kong, Netflix Zuul, Spring cloud gateway, ServiceComb EdgeService등이 있습니다.

  <br>

### Strangler Pattern/스트랭글러 패턴

  ![Strangler Pattern](image.png)
  모놀리틱 아키텍처로 된 시스템이 계속 자라나면, 더 이상 관리할 수 없는 수준이 된다.

  ![Strangler Pattern](image-1.png)

  1. 분리해내야하는 도메인을 Service Layer으로 리팩토링을 해야 한다.
  1. 그리고 분리해낸 도메인의 API가 호출되면, Proxy 단에서 라우팅을 분기처리해준다.
  1. 신규 구축된 서버로 리다이렉트 시켜준다.

### 배포전략 : Rolling, Blue/Green, Canary

  * Rolling
  
    ![Alt text](img1.daumcdn.jpg)

  * Blue/Green

    Blue/Green 패포는 구 버전에서 새 버전으로 일제히 전환하는 전략이다. 구 버전의 서버와 새 버전의 서버들을 동시에 나란히 구성하고 배포 시점이 되면 트랙픽을 일제히 전환시킨다. 하나의 버전만 프로덕션 되므로 버전 관리 문제를 방지할 수 있고, 또한 빠른 롤백이 가능하다. 또 다른 장점으로 운영 환경에 영향을 주지 않고 실제 서비스 환경으로 새 버전 테스트가 가능하다. 예를 들어 구 버전과 새 버전을 모두 구성하고 포트를 다르게 주거나 내부 트랙픽일 경우 새 버전으로 접근하도록 설정하여 테스트를 진행해 볼 수 있다. 단, 시스템 자원이 두 배로 필요하고, 전체 플랫폼에 대한 테스트가 진행 되어야 한다.

    ![Alt text](img1.daumcdn-1.png)


  * Canary

    'Canary'라는 용어의 어원을 알면 이해가 더 쉽다. Canary는 카나리아 라는 새를 일컫는 말인데, 이 새는 일산화탄소 및 유독가스에 매우 민감하다고 한다. 그래서 과거 광부들이 이 새를 옆에 두고 광산에서 일을 하다가 카나리아가 갑자기 죽게 되면 대피를 했다고 한다.

    Canary 배포는 카나리아 새처럼 위험을 빠르게 감지할 수 있는 배포 기법이다. 구 버전의 서버와 새 버전의 서버들을 구성하고 일부 트래픽을 새 버전으로 분산하여 오류 여부를 판단한다. 이 기법으로 A/B 테스트도 가능한데, 오류율 및 성능 모니터링에 유용하다. 트래픽을 분산시킬 라우팅은 랜덤으로 할 수도 있고 사용자 프로필 등을 기반으로 분류할 수도 있다. 분산 후 결과에 따라 새 버전이 운영 환경을 대체할 수도 있고, 다시 구 버전으로 돌아가 수도 있다.

    ![Alt text](img1.daumcdn-2.png)

### 주울에서 경로 구성

주울은 본래 리버스 프로시다. 리버스 프록시는 자원에 접근하려는 클라이언트와 자원 사이에 위치한 중개 서버다. 클라이언트는 프록시가 아닌 다른 서버와 통신하는 것조차 알 수 없고, 리버스 프록시는 클라이언트의 요청을 받은 후 클라이언트를 대신해 원격 자원을 호출한다.

### 서비스 디스커버리를 이용한 수동 경로 매핑

주울을 사용하면 서비스의 유레카 서비스 ID로 자동 생성된 경로에 의존하지 않고 명시적으로 매핑 경로를 정의할 수 있기 때문에 더욱 세분화할 수 있다.

```yaml
zuul:
  # 유레카 서비스 ID 경로 제외(사용자 정의 경로만 사용)
  # 유레가 기반의 경로를 모두 제외하려면 ignored-services: '*'
  ignored-service: 'organizationservice'
  routes:
    organizationservice: /organization/**
```

### 정적 URL을 이용한 수동 경로 매핑

유레카로 관리하지 않는 서비스를 라우팅하는 데 주울을 사용할 수 있다.

```yaml
zuul:
  routes:
    licensestatic:
      path: /licensestatic/**
      url: http://licenseservice-static:8081
```
이때 licensestatic 엔드포인트는 유레카를 사용하지 않고 http://licenseservice-static:8081에 요청을 바로 라우팅한다.

 * 서비스를 여러 경로에 정적으로 매핑
    
    ```yaml
    zuul:
      routes:
        licensestatic:
          path: /licensestatic/**
          serviceid: licensestatic
    ribbon:
      eureka:
        enabled: false  # 리본에서 유레카 지원을 끈다.
    licenstatic:
      ribbon:
        listOfServers: http://licenseservice-static1:8081,
          http://licenseservice-static2:8082
    ```

> JVM 기반이 아닌 서비스 다루기  
  스프링 클라우드의 **스프링 사이드카** 인스턴스를 설정

### 주울과 서비스 타임아웃

주울은 넷플릭스의 히스트릭스와 리본 라이브러리를 사용해 오래 수행되는 서비스 호출이 서비스게이트웨dl의 성능에 영향을 미치지 않도록 한다.

```yaml
# 모든 서비스에 히스트릭스 타임아웃을 설정
hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds: 2500

# 특정 서비스(licensingservice)에 별도의 히스트릭스 타임아웃을 설정
hystrix.command.licensingservice.execution.isolation.thread.timeoutInMillisecounds: 3000

# 리본의 타임아웃을 재정의
licensingservice.ribbon.ReadTimeout: 7000
```

### 주울의 진정한 힘! 필터

주울 게이트웨이로 유입되는 모든 요청을 프록시해서 서비스 호출을 단순화할 수 있지만, 주울의 진정한 능력은 게이트웨이를 통과하는 모든 서비스 호출에 대해 사용자 정의 로직을 작설할 때 드러난다.

주울은 유입되는 요청에 직접 HTTP 요청 헤더를 추가하거나 수정하는 것을 금한다.  
ZuulRequestHeader 맵에 보관된 데이터는 주울 서버가 대상 서비스를 호출할 대 합쳐진다.

 * 상관관계 ID의 전파를 보장하는 사용자 정의 RestTemplate와 UserContextInterceptor

   RestTemplate인스턴스에서 실행되는 HTTP 기반 발신요청에 상관관계 ID를 삽이하여, 서비스 호출 간 연결을 형성

   UserContextInterceptor를 사용하려면 RestTemplate 빈을 정의한 후 UserContextInterceptor를 그 빈에 추가해야 한다.

   ```java
   @Bean
   @LoadBalaced // 이 RestTemplate 객체가 리본을 사용할 것을 나타낸다.
   public RestTemplate getRestTemplate() {
     RestTemplate template = new RestTemplate();
     List interceptors = template.getInterceptors();
     if (interceptors = null) {
       template.setInterceptors(Collections.singletonList(new UserContextInterceptor()));
     } else {
       interceptors.add(new UserContextInterceptor());
       template.setInterceptors(interceptors);
     }
     return template;
   }
   ```

### 동적 경로 필터 작성

   ```bash
   $> docker ps | grep postgres
   $> docker exec -it {CONTAINER ID} /bin/sh
    > psql -U postgres
    $ \c eagle_eye_local
    $ SELECT * FROM abtesting;
   ```

* WebClient

  Spring Framework 5부터 WebFlux 스택과 함께 Spring은 WebClient 라는 새로운 HTTP 클라이언트를 도입했습니다.  
  WebClient 는 RestTemplate에 대한 최신 대체 HTTP 클라이언트 입니다. 기존의 동기식 API를 제공 할뿐만 아니라 효율적인 비 차단 및 비동기 접근 방식도 지원합니다.


## spmia-chapter7

> 마이크로서비스의 보안  
  https://github.com/klimtever/spmia-chapter7.git

스프링 시큐리티를 적용하는 과정에서 jwt 토큰 생성 중 발생한 에러이다  
해당 문제는 jdk 11 이상 버전에서는 관련 모듈이 기본 참조되지 않아 에러가 발생

 * zuulsvr/pom.xml

   ```xml
   <!-- https://mvnrepository.com/artifact/javax.xml.bind/jaxb-api -->
   <dependency>
     <groupId>javax.xml.bind</groupId>
     <artifactId>jaxb-api</artifactId>
     <version>2.3.1</version>
   </dependency>
   ```

 * 빌드 및 실행

   ```
   $> mvc clean package docker:build

   $> sudo apt purge --auto-remove redis-server
   $> docker container rm -f $(docker container list -qa)

   $> docker-compose -f docker/common/docker-compose.yml up -d
   $> docker-compose -f docker/common/docker-compose.yml down

   $> docker images rm -rf $(docker images -qa)
   ```

 * launch.json(VSCode 디버깅)

    ```shell
    {
      "type": "java",
      "name": "authentication-service",
      "request": "launch",
      "mainClass": "com.thoughtmechanix.authentication.Application",
      "projectName": "authentication-service",
      "vmArgs":[
          "-Djava.security.egd=file:/dev/./urandom",
          "-Dserver.port=5051",
          "-Deureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka/"
      ]
    }
    ```


# 서비스 디스커버리

 * 유레카에 등록된 서비스 목록을 확인할 수 있따.

   ```http
   GET http://192.168.250.99:8761/eureka/apps HTTP/1.1
   ```

## 디스커버리 클라이언트 - 유레카 클라이언트

 유레카 클라이언트는 다음의 두가지 역할을 수행한다.

   * 유레카 서버와 통신하여 자신을 등록하고 서비스 정보를 보낸다.  

   * 서버로부터 테이터를 가져와 캐싱하고 주기적으로 변경사항을 확인한다.

 ****유레카 클라이언트 활성화****

 유레카 클라이언트를 활성화하기 위한 애노테이션은 다음의 두개 중 하나를 사용

   * org.springframework.cloud.client.discovery.EnableDiscoveryClient

   * org.springframework.cloud.netflix.eureka.EnableEurekaClient

  ```shell
  # 빌드
  ./gradlew -x test clean build
  
  # 실행
  java -jar -Dserver.port=8081 study-eureka-client-0.0.1-SNAPSHOT.jar
  java -jar -Dserver.port=8082 study-eureka-client-0.0.1-SNAPSHOT.jar
  ```
## 서비스 디스커버리를 사용해 서비스 검색


# 스프링 클라우드 슬루스와 집킨을 이용한 분산 추적

```shell
$> mvn clean package docker:build

$> docker-compose -f docker/common/docker-compose.yml up
```

# Linux 자바 설치

docker ps rm -f $(docker ps -qa)


docker-compose -f docker-compose.yml up -d

## Docker로 Kafka 설치하기

```yaml
version: '2'
services:
  zookeeper:
    container_name: zookeeper
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"
  kafka:
    container_name: kafka
    image: wurstmeister/kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: 127.0.0.1
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_CREATE_TOPICS: "test-topic:1:1"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
```

```shell
# 컨테이너 생성 / 실행
$> docker-compose -f docker-compose.yml up -d
$> docker ps -a

# Docker kafka container 접속
$> docker exec it [컨테이너명] /bin/bash
```

```shell
# Kafka Topic 생성
$> kafka-topics.sh --create --topic sample-topic --bootstrap-server localhost:9092 \
   --replication-factor 1 --partitions 1
```   

 * kafka-topics.sh : kafka-topics.sh shell script  
 -- create: topic 생성  
 -- topic : topic 명  
 -- bootstrap-server: host:port, 대상 카프가 클러스터
 -- replication-factor : replica 갯수 지정(복제본의 수)
 -- partitions: partitions 갯수 지정(토픽을 몇개로 나눌 것인가)

```shell
# topic 확인
$> kafka-topics.sh --list --bootstrap-server localhost:9082

# Producer - 메시지 전송
$> kafka-console-producer.sh --topic sample-topic --bootstrap-server localhost:9002

# Consumer - 메시지 읽기
$> kafka-console-consumer.sh --topic sample-topic --bootstrap-server localhost:9092 --from-beginning
```   

## nc 명령어

TCP 또는 UDP 프로토콜을 사용하는 네트워크 환경에서 데이터를 읽고 쓰는 간단한 프로그램.  
일반적으로 상대 서버의 포트가 열렸는지 확인하거나, 직접 서버가 되어 원격 서버에서 (클라이언트) 접속이 가능한지 확인하는 용도로 사용을 한다.

| 옵션 | 설명 | 옵션 | 설명 |
| --- | --- | --- | --- |
| -u  | udp 연결 | -p | 소스 포트 지정 |
| -ㅣ | LISTEN 모드로 포트 띄움 | -z | 단순 포트 스캔만 진행 |
| -v | 더 많은 정보 확인 |

### docker-proxy

이 프로세스의 목적은 그 이름처럼 docker host 로 들어온 요청을 container 로 넘기는 것 뿐이다.  
docker-proxy 는 kernel이 아닌 userland에서 수행되기 때문에 kernel과 상관없이 host가 받은 패킷을 그대로 container의 port로 넘긴다.

### Pod To Service

 > kube-proxy의 netfilter에 정의되어 있는 chain rule에 의하여 요청을 포워딩한다.

쿠버네티스는 기본적으로 Pod는 쉽게 대체될 수 있는 존재이기 때문에 IP로 Service와 통신하기에는 부적절하다. 따라서 Pod앞단에 Reverse Proxy를 위치시키는 방법이있다. 이 Reverse Proxy를 수행해주는 추상적인 리소스가 Service이다.



* java.security.egd

  java.security.egd는 자바의 SecureRandom 클래스를 초기화하는 방식에 영향을 미친다.

  난수를 생성한다던지 등의 목적으로 최초로 SecureRandom을 이용할 때 JVM의 java.security의 구성파일을 초기화 하고 읽게 되는데 해당 초기화를 어떻게 진행할지에 대한 설정 값이다.



 * HystrixCommand

   @HystrixCommand의 1초 timeout 기본옵션

   ```java
   @HystrixCommand(
       commandProperties = {
           @HystrixProperty(name="execution.isolation.thread.timeoutInMilliseconds", value="1")
           @HystrixProperty(name="circuitBreaker.requestVolumeThreshold", value="10")
       },
       fallbackMethod = "sampleFallback"
   )
   @RequestMapping("/hello")
   public String printHelloWorld() {

       String result = restTemplate.getForObject("http://eurekaclient2/hello", String.class);
       return result;
   }

   private String sampleFallback() {
       return "circuit breaker on";
   }
   ```
   위와 같은 방식으로 @HystrixCommand를 변형하여 property를 사용할 수 있다.

 * spring boot start

    ```bash
    $> curl https://start.spring.io/starter.tgz -d dependencies=security,data-jpa,h2,data-rest,lombok -d baseDir=oauth2-server-sample | tar -xzvf - #baseDir 디렉터리에서 생성

    $> cd oauth2-server-sample
    $> mvn clean spring-boot:run
    ```

* OAuth2 sample
  ```baseh
  $> git clone -b example6 https://github.com/sbcoba/spring-boot-oauth2-sample.git example6

    $> git clone -b example8 https://github.com/sbcoba/spring-boot-oauth2-sample.git example8
  ```

![Alt text](img1.daumcdn-1.jpg)

OAuth2 프로세스

* OAuth2 권한 서버 설정

  @EnableAuthorizationServer 어노테이션은 OAuth2의 인증 처리 권한을 갖는 서버로 사용하겠다는 뜻 /oauth/token 과 /oauth/authorize 를 포함한 여러 endpoints가 자동으로 생성됨

- ClientDetailsServiceConfigurer

### [springboot-oauth2 sample](https://github.com/cheese10yun/springboot-oauth2.git)

  > [Spring OAuth2 Provider 정리](https://cheese10yun.github.io/spring-oauth2-provider/)

  * [sample](https://github.com/cheese10yun/springboot-oauth2.git) 실행 환경

     ```bash
     # 도커에서 mysql 이미지 버전 확인
     $> docker search mysql

     $> docker pull mysql/mysql-server:5.7
     $> docker pull --platform linux/amd64 mysql/mysql-server:5.7
     ```

  - 인증

    > http://localhost:8080/oauth/authorize?client_id=client&redirect_uri=http://localhost:9000/callback&response_type=code&scope=read_profile


#### Spring Security OAuth2의 기본 TokenStore 종류

 * org.springframework.security.oauth2.provider.token.store.InMemoryTokenStore
 * org.springframework.security.oauth2.provider.token.store.JdbcTokenStore
 * org.springframework.security.oauth2.provider.token.store.JwtTokenStore
 * org.springframework.security.oauth2.provider.token.store.RedisTokenStore

#### JdbcTokenStore를 구현하기 사전 준비

[DB 스키마](https://github.com/spring-attic/spring-security-oauth/blob/main/spring-security-oauth2/src/test/resources/schema.sql)

```bash
# SAMPLE
$> git clone -b example6 https://github.com/sbcoba/spring-boot-oauth2-sample.git example6
$> cd example6

# Oauth2 서버 실행 (포트 8080)
$> mvn clean -pl oauth2-server spring-boot:run &

# API 서버 실행 (포트 8081)
$> mvn clean -pl api-server spring-boot:run &

# API 서버 실행 (포트 8081)
$> mvn clean -pl api-server spring-boot:run -Dserver.port=9999 &

# Access Token 발급
$> curl -F "grant_type=client_credentials" -F "scope=read" http:foo:bar@localhost:8080/oauth/token

# Access Token 과 함께 API 서버에서 API 호울
$> curl -H "Authorization: Bearer {Access Token}" "http://localhost:8081/members"
```

* a

   ```yaml
   # token 검증 서버 분리
   security.oauth2.authorization.check-token-access: isAuthenticated()

   security.oauth2.resource.token-info-uri: http://localhost:8080/oauth/check_token

   # spring-security-jwt 종속성 설정이후
   # 서명할 key 값 필요
   security.oauth2.authorization.token-key-access: isAuthenticated()

   security.oauth2.jwt.key-uri: http://localhost:8080/oauth/token_key
   ```

#### OAuth2 승인 방식의 종류
 * Authorization Code Grant
 * Implicit Grant
 * Resource Owner Password Credentials Grant
 * Client Credentials Grant

##### Authorization Code Grant

![Alt text](oauth2-doe-grant-type_gnojt19me.png)

## spmia-chapter8

  > 스프링 클라우드 스트림을 사용한 이벤트 기반 아키텍처

## spmia-chapter9

  > 스프링 클라우드 슬루스와 집킨을 이용한 분산 추적  
    https://github.com/klimtever/spmia-chapter9.git

  * 스프링 클라우드 슬루스를 사용해 서비스 호출에 추적 정보 주입
  * 로그를 수집해 분산 트랜잭션 로그 보기
  * 로그 수집 도구에서 질의
  * 오픈집킨(OpenSipkin)을 사용해 마이크로서비스 호출 사이의 사용자 트랜잭션을 시각적으로 이해
  * 스프링 클라우드 슬루스와 집킨으로 추적 정보를 사용자 정의

### 분산 추적 디버깅

마이크로서비스는 본질적으로 분산되어 있기 떄문에 문제가 발생한 위치를 디버깅하는 것은 매우 번거로운 작업이다. 분산된 서비스의 특징은 여러 서비스와 물리 머신, 다양한 데이터 저장소에 걸쳐 단일 또는 복수 트랜잭션을 추적한 후 정확히 상황을 종합하려고 노력해야 한다는 것이다.


### 스프링 클라우드 슬루스와 상관관계 ID

